{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Intro_Machine_Learning_part1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiPJp5P2GKCR",
        "colab_type": "text"
      },
      "source": [
        "# Introduction to Machine Learning\n",
        "\n",
        "This course provides an introduction on machine learning. It gives an overview of many concepts, techniques and algorithms in machine learning, beginning with topics such as classification and linear regression and ending up with more recent topics such support vector machines. The course is divided into three main topics: supervised learning, unsupervised learning, and machine learning theory. Topics include: (i) Supervised learning (linear decision, non linear decision and probabilistic). (ii) Unsupervised learning (clustering, factor analysis, visualization). (iii) Learning theory (bias/variance theory, empirical risk minimization). The course will also draw from numerous case studies and applications, so that you'll also learn how to apply learning algorithms to computer vision, medical informatics, and signal analysis [1].\n",
        "\n",
        "[1] https://www.fib.upc.edu/en/studies/masters/master-artificial-intelligence/curriculum/syllabus/IML-MAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNj2E14VGdvT",
        "colab_type": "text"
      },
      "source": [
        "## Contents\n",
        "\n",
        "1. **Introduction to machine learning**\n",
        "  - What is learning?\n",
        "  - Definition of learning\n",
        "  - Elements of machine learning\n",
        "  - Paradigms of machine learning \n",
        "  - Applications of machine learning\n",
        "  - Nuts and bolts of machine learning theory\n",
        "\n",
        "2. Unsupervised learning  \n",
        "  - Introduction to unsupervised learning\n",
        "  - Clustering\n",
        "  - Classification of clustering algorithms: K-Means and EM \n",
        "  - Factor Analysis : PCA (Principal Components Analysis) and ICA (Independent Component Analysis)\n",
        "  - Self-Organized Maps (SOM) and Multi-dimensional Scaling\n",
        "  - Recommender Systems\n",
        "\n",
        "3. Supervised learning \n",
        "  - Introduction and perspectives\n",
        "  - Lazy Learning \n",
        "  - Introduction to feature selection \n",
        "  - Model selection \n",
        "  - Supervised learning taxonomy\n",
        "  - Linear decision\n",
        "  - Non-linear decision learning: Kernel methods\n",
        "  - Non-linear decision learning: Ensemble Learning\n",
        "  - Bayesian Learning\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wn3XZcpdHhHD",
        "colab_type": "text"
      },
      "source": [
        "## Bibliography\n",
        "**Basic:**\n",
        "\n",
        "* Pattern recognition and machine learning - Bishop, Christopher M, Springer, cop. 2006. \n",
        "\n",
        "* Machine learning - Mitchell, Tom M, The McGraw-Hill Companies, cop. 1997. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6ui4PodKf66",
        "colab_type": "text"
      },
      "source": [
        "## 1. Introduction to ML\n",
        "\n",
        "**Learning**\n",
        "\n",
        "- Learning: to improve automatically with experience.\n",
        "- Learning: process of converting experience into expertise of knowledge [2].\n",
        "\n",
        "- Definition of learning:\n",
        "  - A computer pogram is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured in P, improves with experience E [1].\n",
        "\n",
        "  \n",
        "  - For instance: handwriting recognition problem:\n",
        "    - E (experience): dataset of handwritten words with given classifications.\n",
        "    - T (Task): Recognize handwritten words within images.\n",
        "    - P (Performance): Percent of words correctly classified.\n",
        "    \n",
        "- The learner's task is to search over a vast space of possible hypotheses to locate the hypothesis that is most consistent with the training examples.\n",
        "\n",
        "- A succesful learner should be able to progress from individual examples to broader generalization. This learning (in contrast to learning by memorization) is also referred to as inductive reasoning or inductive inference.\n",
        "\n",
        "- It is important to consider that when multiple hypotheses explain something equally well, we should choose the simplest one. **Occam's razor**. But also taking into account what **Albert einstein** said: Eveything should be made as simple as possible, but not simpler. Finally, there is no universally best model, **no free lunch theorem** (Wolpert, 1996), because a set of assumptions that works well in one domain may work poorly in another.\n",
        "  \n",
        "**Why do wee need Machine Learning**\n",
        "\n",
        "Task that are too complex to program:\n",
        "\n",
        "- Tasks performed by Animals/Humans: For instance, driving, speech recognition, image understanding, etc.\n",
        "\n",
        "- Task beyond human capabilities: analysis of very large and complex datasets.\n",
        "  \n",
        "Adaptivity:\n",
        "\n",
        "- A normal program is written only for some specifics functions, not more. It stays unchanged.\n",
        "- A ML adapts its behavior to their input data according to changes in the environment they iteract with.\n",
        "\n",
        "**Paradigms of machine learning**\n",
        "\n",
        "- Supervised (predictive) Learning: an agent observes an input-output pairs that learns a function that maps from input to output [3]. The idea:\n",
        "  - Given a training set: $\\left(x_{1}, y_{1}\\right),\\left(x_{2}, y_{2}\\right), \\ldots\\left(x_{N}, y_{N}\\right)$, where $y_j$ was generated by an unknown function $y = f(x)$, the task is to discover a function $h$ select from a hypothesis space, $\\mathcal{H}$,  that approximates the true function $f$ (sometimes this function is stochastic, so it is not strictly a function of x, instead we have $P(Y|x)$). $h$ is a hypothesis. If $y$ is a set of finite values, then the learning problem is called classification. When $y$ is real number it is called regression. \n",
        "  - To sum up, supervised learning is about choosing $h^*$ that is most probable to fit the data: $h^{*}=\\underset{h \\in \\mathcal{H}}{\\operatorname{argmax}} P(h | d a t a)$.\n",
        "- Unsupervised Learning: the agent learns patterns in the input without explicit labels. The idea is to map a feature vector $x=\\left\\{\\mathbf{x}_{i}\\right\\}_{i=1}^{N}$ into another vector or value, depending of the problem. For instance, if it is a clustering problem, the model return a number which is the cluster where the points (input feature vector) belong. As for dimensionality reductiorn, the output if a feature vector with a reduced features (reduce dimension). In outlier detection, the output is a real number describing how the feature vector is different from a normal example in the dataset [4].\n",
        "\n",
        "- Reinforcement Learning: the agent learns from a series of reinforcements - rewards or punishments.\n",
        "  \n",
        "  \n",
        "[1] Mitchell, Tom M. Machine Learning, 1997.\n",
        "\n",
        "[2] Shai Shalev at el. Understanding ML theory. 2014.\n",
        "\n",
        "[3] Stuart Russell, Peter Norving. AI a modern approach. 2010.\n",
        "\n",
        "[4] Andriy Burkov. The Hundred-Page Machine Learning Book . 2019.\n"
      ]
    }
  ]
}