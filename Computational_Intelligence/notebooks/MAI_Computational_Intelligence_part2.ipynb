{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MAI_Computational_Intelligence_part2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiPJp5P2GKCR",
        "colab_type": "text"
      },
      "source": [
        "# Computational Intelligence\n",
        "\n",
        "The aim of this course is to provide the students with the knowledge and skills required to design and implement effective and efficient Computational Intelligence solutions to problems for which a direct solution is impractical or unknown. Specifically, students will acquire the basic concepts of fuzzy, evolutionary and neural computation. The student will also apply this knowledge to solve some real case studies. [1].\n",
        "\n",
        "[1] https://www.fib.upc.edu/en/studies/masters/master-artificial-intelligence/curriculum/syllabus/CI-MAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNj2E14VGdvT",
        "colab_type": "text"
      },
      "source": [
        "## Contents\n",
        "\n",
        "1. Introduction to Computational Intelligence \n",
        "  - Computational Intelligence: definition and paradigms. Brief historical sketch.\n",
        "2. **Foundations of Neural Computation** \n",
        "  - Introduction to neural computation: biological inspiration, neural network models, architectures and training algorithms. Learning and generalization.\n",
        "3. Foundations of Evolutionary Computation \n",
        "  - Introduction to evolutionary computation: evolutionary processes in nature, genetic operators, evolutionary optimization algorithms. Genetic algorithms. Evolution Strategies and CMA-ES.\n",
        "4. Foundations of Fuzzy Computation \n",
        "  - Introduction to fuzzy computation: fuzzy sets and systems, fuzzy inference systems and FIR.\n",
        "5. Applications and case studies \n",
        "  - Applications and case studies on real problems in regression, classification, identification and system optimization\n",
        "\n",
        "Chosen project: GANs for data augmentation for updating a machine learning model.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wn3XZcpdHhHD",
        "colab_type": "text"
      },
      "source": [
        "## Bibliography\n",
        "**Basic:**\n",
        "\n",
        "* **Computational Intelligence: An Introduction - Engelbrecht, Andries, Wiley, 2007** .\n",
        "\n",
        "* **Neural networks and learning machines - Haykin, Simon S, Prentice Hall, cop. 2009.**\n",
        "\n",
        "**Website:**\n",
        "  \n",
        "* http://ci.cs.up.ac.za/?page=algo_pso"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhSu5ZQAwU-B",
        "colab_type": "text"
      },
      "source": [
        "## 2. Foundations of Neural Computation\n",
        "\n",
        "- Introduction to neural computation: biological inspiration, neural network models, architectures and training algorithms. Learning and generalization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wD0_DEEDB2tU",
        "colab_type": "text"
      },
      "source": [
        "### What is a neural network?\n",
        "\n",
        "\n",
        "- A Neural Network is a massively parallel distributed processor, made up of processing units that store experiential knowledge. Knowledge is acquired by a learning process. This acquired knowledge is stored in weights.\n",
        "\n",
        "**Benefits of NN**\n",
        "\n",
        "- **Nonlinearity**: Deals with complex problems with nonlinear boundaries (classification).\n",
        "- **Input-Output Mapping**: Also known as supervised learning that involves modifying the weights of a NN by using the training samples (x, y) = (input, target). It also is considered as nonparametric model, also a model-free estimation, tabula rasa learning because no prior assumptions are made on a statistical model for the input data. For instance, the estimation of decision boundaries are not based on statistical distribution such as  normal, poisson, bernoulli, etc.\n",
        "- **Adaptivity**: It can change their weights in real time base on environmental conditions.\n",
        "- **Evidential Response**: It shows the confidence in the decision made, it is like the probability value when dealing with classification problems. It is a measure also of the network's performance.\n",
        "- **Contextual Information**: Every neuron is affected byt the global activity of the other neurons in NN.\n",
        "- **Fault Tolerance.**: It is robust or its performance degrades gracefully under adverse operating conditions. For instance, if some neurons are failing, the degradation is slow.\n",
        "- **very-large-scale-integrated Implementability.**: Due to its parallel nature it is fast for the computation of tasks.\n",
        "\n",
        "\n",
        "**Model of Neuron**\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1-OzYb6o0DnsDHkGiO59KFsePPeuxx12w\" alt=\"drawing\" width=\"500\" />\n",
        "\n",
        "\n",
        "$$u_{k}=\\sum_{j=1}^{m} w_{k j} x_{j}$$\n",
        "\n",
        "$$y_{k}=\\varphi\\left(u_{k}+b_{k}\\right)$$\n",
        "\n",
        "where $x_{1}, x_{2}, \\dots, x_{m}$ are the inputs and $w_{k 1}, w_{k 2}, \\ldots, w_{k m}$ the weights of neurons. $u_k$ is the linear combiner output. $b_k$ is the bias and $\\varphi(\\cdot)$ is the activation function. $y_k$ is the output of the activation function (the prediction). The goal of the bias is to apply an affine transformation to the $u_k$:\n",
        "\n",
        "$$\n",
        "v_{k}=u_{k}+b_{k}\n",
        "$$\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1sIKDYUv2_Ag4uBiFzCg3dRJ-QSHuRt-K\" alt=\"drawing\" width=\"500\" />\n",
        "\n",
        "### Network architectures\n",
        "\n",
        "**Single-Layer Feedforward Networks**\n",
        "\n",
        "Only one layer, in one direction.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1rDu6fiviJVCKoii4bHIMQ-DuhbgfEeS5\" alt=\"drawing\" width=\"400\" />\n",
        "\n",
        "**Multilayer Feedforward Networks**\n",
        "\n",
        "One or more hidden layers. The layers are called hidden because they are between the input and output layer. The more hidden layer, the higher order of complexity. The NN acquires a globla perspective against a local conectivity. The following example shows a 10-4-2 network, 10 input nodes, 4 hidden neurons and 2 output neurons.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1oAeiPOd1jJdZRvYB2emO-HrGHUmDEVCh\" alt=\"drawing\" width=\"400\" />\n",
        "\n",
        "\n",
        "**Recurrent Networks**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nYe9dCkiPO2",
        "colab_type": "text"
      },
      "source": [
        "## Rosenblatt's Perceptron\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWzZS7I8wWHV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}